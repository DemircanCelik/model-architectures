{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6923c3ed325044c3bf40f798e496b096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, Gemma2ForCausalLM\n",
    "\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", use_fast=True)\n",
    "tr_tokenizer = AutoTokenizer.from_pretrained(\"alibayram/tr_tokenizer\", use_fast=True)\n",
    "hf_model = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 235300,  25725,   1560,   4119, 235269,  42233,   2586,    473,\n",
       "         235336, 102823, 153384,    972,    533, 173077, 235341]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = \"Nasılsın, iyi misin? Türkçe cevap ver lütfen!\"\n",
    "input_ids = hf_tokenizer.encode(example_text, return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     2, 235300,  25725,   1560,   4119, 235269,  42233,   2586,    473,\n",
       "          235336, 102823, 153384,    972,    533, 173077, 235341,    109,    688,\n",
       "          178229, 235341,   4658,    571, 190404,  14630,  56292, 235265,    139,\n",
       "           13625,   7863, 235405,   1560,   4119, 235336, 206506,   2586,    473,\n",
       "          235336,    688,    109,    688,   7510,  42233,  89249, 235269, 162625,\n",
       "          165296, 235265,   5620,   7863, 235405,   1560,   4119, 235336,    688,\n",
       "             109,    688, 235070,  89249, 235269, 162625, 165296, 235265,    139,\n",
       "           13625,    679,  18168, 182564,    549, 235336,    688,    109,    688,\n",
       "            7510,    581,   2843,  15128, 122469,  89249, 235269, 102823,  80781,\n",
       "           21032,  44875, 235265,    139,  13625,    679,  18168, 182564,    549,\n",
       "          235336,    688,    109,    688,   7510,    581, 102823,  80781,  21032,\n",
       "           44875]]),\n",
       " '<bos>Nasılsın, iyi misin? Türkçe cevap ver lütfen!\\n\\n**Merhaba! Benim adım Ayşe.  Sen nasılsın? İyi misin?**\\n\\n**Ben iyiyim, teşekkür ederim. Sen nasılsın?**\\n\\n**İyiyim, teşekkür ederim.  Sen ne yapıyorsun?**\\n\\n**Ben de bir dil modeliyim, Türkçe konuşabiliyorum.  Sen ne yapıyorsun?**\\n\\n**Ben de Türkçe konuşabiliyorum')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before changing of the embedding layer\n",
    "output_token_ids = hf_model.generate(input_ids, max_length=100)\n",
    "output_token_ids, hf_tokenizer.decode(output_token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.lm_head.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.reduction import ForkingPickler\n",
    "from types import FunctionType\n",
    "import cloudpickle\n",
    "\n",
    "assert sys.version_info >= (3, 8), 'python3.8 or greater required to use reducer_override'\n",
    "\n",
    "def reducer_override(obj):\n",
    "    if type(obj) is FunctionType:\n",
    "        return (cloudpickle.loads, (cloudpickle.dumps(obj),))\n",
    "    else:\n",
    "        return NotImplemented\n",
    "\n",
    "# Monkeypatch our function reducer into the pickler for multiprocessing.\n",
    "# Without this line, the main block will not work on windows or macOS.\n",
    "# Alterntively, moving the defintionn of foo outside of the if statement\n",
    "# would make the main block work on windows or macOS (when run from\n",
    "# the command line).\n",
    "ForkingPickler.reducer_override = staticmethod(reducer_override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, ['özellik', 'Tekmele', 'ejder', 'irdele', 'akordeon'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize = 1000\n",
    "\n",
    "chunks = [list(tr_tokenizer.vocab.keys())[i:i + chunksize] for i in range(0, len(tr_tokenizer.vocab), chunksize)]\n",
    "len(chunks), chunks[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "if __name__ == '__main__':\n",
    "  def find_tokens_map(tr_tokens):\n",
    "    global counter\n",
    "    token_maps = []\n",
    "    for tr_token in tr_tokens:\n",
    "        tr_token_id = tr_tokenizer.vocab[tr_token]\n",
    "        cosmos_token_ids = hf_tokenizer.encode(tr_token)\n",
    "\n",
    "        token_maps.append({\n",
    "          \"tr_token\": tr_token,\n",
    "          \"tr_token_id\": tr_token_id,\n",
    "          \"hf_token_ids\": cosmos_token_ids\n",
    "          })\n",
    "        counter += 1\n",
    "        print(counter, tr_token, tr_token_id, len(cosmos_token_ids))\n",
    "    return token_maps\n",
    "  \n",
    "  with Pool(100) as p:\n",
    "    token_maps_list = p.map(find_tokens_map, chunks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tr_token</th>\n",
       "      <th>tr_token_id</th>\n",
       "      <th>hf_token_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>özellik</td>\n",
       "      <td>3095</td>\n",
       "      <td>[2, 235397, 6085, 3871]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tekmele</td>\n",
       "      <td>26542</td>\n",
       "      <td>[2, 36764, 42030]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ejder</td>\n",
       "      <td>11233</td>\n",
       "      <td>[2, 15117, 866]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>irdele</td>\n",
       "      <td>7765</td>\n",
       "      <td>[2, 616, 37725]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>akordeon</td>\n",
       "      <td>26977</td>\n",
       "      <td>[2, 738, 29141, 477]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30153</th>\n",
       "      <td>bekri</td>\n",
       "      <td>26934</td>\n",
       "      <td>[2, 18608, 505]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30154</th>\n",
       "      <td>elan</td>\n",
       "      <td>16290</td>\n",
       "      <td>[2, 89241]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30155</th>\n",
       "      <td>model</td>\n",
       "      <td>3164</td>\n",
       "      <td>[2, 2516]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30156</th>\n",
       "      <td>Muti</td>\n",
       "      <td>16975</td>\n",
       "      <td>[2, 235296, 2749]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30157</th>\n",
       "      <td>Ebru</td>\n",
       "      <td>22284</td>\n",
       "      <td>[2, 235291, 10335]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30158 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tr_token  tr_token_id             hf_token_ids\n",
       "0       özellik         3095  [2, 235397, 6085, 3871]\n",
       "1       Tekmele        26542        [2, 36764, 42030]\n",
       "2         ejder        11233          [2, 15117, 866]\n",
       "3        irdele         7765          [2, 616, 37725]\n",
       "4      akordeon        26977     [2, 738, 29141, 477]\n",
       "...         ...          ...                      ...\n",
       "30153     bekri        26934          [2, 18608, 505]\n",
       "30154      elan        16290               [2, 89241]\n",
       "30155     model         3164                [2, 2516]\n",
       "30156      Muti        16975        [2, 235296, 2749]\n",
       "30157      Ebru        22284       [2, 235291, 10335]\n",
       "\n",
       "[30158 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "token_list_for_df = []\n",
    "for token_maps in token_maps_list:\n",
    "  for token_map in token_maps:\n",
    "    token_list_for_df.append(token_map)\n",
    "\n",
    "df = pd.DataFrame(token_list_for_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tr_token</th>\n",
       "      <th>tr_token_id</th>\n",
       "      <th>hf_token_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>özellik</td>\n",
       "      <td>3095</td>\n",
       "      <td>[235397, 6085, 3871]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tekmele</td>\n",
       "      <td>26542</td>\n",
       "      <td>[36764, 42030]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ejder</td>\n",
       "      <td>11233</td>\n",
       "      <td>[15117, 866]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>irdele</td>\n",
       "      <td>7765</td>\n",
       "      <td>[616, 37725]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>akordeon</td>\n",
       "      <td>26977</td>\n",
       "      <td>[738, 29141, 477]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30153</th>\n",
       "      <td>bekri</td>\n",
       "      <td>26934</td>\n",
       "      <td>[18608, 505]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30154</th>\n",
       "      <td>elan</td>\n",
       "      <td>16290</td>\n",
       "      <td>[89241]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30155</th>\n",
       "      <td>model</td>\n",
       "      <td>3164</td>\n",
       "      <td>[2516]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30156</th>\n",
       "      <td>Muti</td>\n",
       "      <td>16975</td>\n",
       "      <td>[235296, 2749]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30157</th>\n",
       "      <td>Ebru</td>\n",
       "      <td>22284</td>\n",
       "      <td>[235291, 10335]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30158 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tr_token  tr_token_id          hf_token_ids\n",
       "0       özellik         3095  [235397, 6085, 3871]\n",
       "1       Tekmele        26542        [36764, 42030]\n",
       "2         ejder        11233          [15117, 866]\n",
       "3        irdele         7765          [616, 37725]\n",
       "4      akordeon        26977     [738, 29141, 477]\n",
       "...         ...          ...                   ...\n",
       "30153     bekri        26934          [18608, 505]\n",
       "30154      elan        16290               [89241]\n",
       "30155     model         3164                [2516]\n",
       "30156      Muti        16975        [235296, 2749]\n",
       "30157      Ebru        22284       [235291, 10335]\n",
       "\n",
       "[30158 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove first token from llama_token_ids and gemma2_token_ids\n",
    "df[\"hf_token_ids\"] = df[\"hf_token_ids\"].apply(lambda x: x[1:])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.lm_head.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.nn.parameter.Parameter)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "gemma_embeddings = hf_model.lm_head.weight\n",
    "embeddings = torch.zeros(len(df), gemma_embeddings.shape[1])# torch.nn.parameter.Parameter(gemma_embeddings[:len(df)]) \n",
    "# torch.zeros(len(df), gemma_embeddings.shape[1])\n",
    "type(embeddings), type(gemma_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change first token embeddings\n",
    "# embeddings[0] = gemma_embeddings[0]\n",
    "# RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" for token_map in token_list:\n",
    "    index = token_map['tr_token_id']\n",
    "    cosmos_token_ids = token_map['cosmos_token_ids']\n",
    "    embedding = cosmos_embeddings[cosmos_token_ids[0]]\n",
    "    sum_embedding = embedding\n",
    "    for cosmos_token_id in cosmos_token_ids[1:]:\n",
    "        embedding = embedding + cosmos_embeddings[cosmos_token_id]\n",
    "    if len(cosmos_token_ids) > 1:\n",
    "        embedding = embedding / len(cosmos_token_ids)        \n",
    "    embeddings[index] = embedding\n",
    "\n",
    "embeddings[0] \"\"\"\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    tr_token_id = row[\"tr_token_id\"]\n",
    "    hf_token_ids = row[\"hf_token_ids\"]\n",
    "    embedding = gemma_embeddings[hf_token_ids[0]]\n",
    "    sum_embedding = embedding\n",
    "    for hf_token_id in hf_token_ids[1:]:\n",
    "        embedding = embedding + gemma_embeddings[hf_token_id]\n",
    "    if len(hf_token_ids) > 1:\n",
    "        embedding = embedding / len(hf_token_ids)        \n",
    "    embeddings[i] = embedding\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.parameter.Parameter, True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_embedding = torch.nn.Parameter(embeddings)\n",
    "type(p_embedding), p_embedding.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30158, 2304])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the embedding layer\n",
    "hf_model.lm_head.weight = p_embedding\n",
    "hf_model.lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12312,  6055,    17,  2018,  6402,  1209,    36,  5813,    80,  1630,\n",
       "         28927,  1344, 26387,     6]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = \"Nasılsın, iyi misin? Türkçe cevap ver lütfen!\"\n",
    "input_ids = tr_tokenizer.encode(example_text, return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after changing of the embedding layer\n",
    "output_token_ids = hf_model.generate(input_ids, max_length=100)\n",
    "output_token_ids, tr_tokenizer.decode(output_token_ids[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
