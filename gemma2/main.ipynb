{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alibayram/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "GemmaForCausalLM(\n",
    "  (embedder): Embedding()\n",
    "  (model): GemmaModel(\n",
    "    (layers): ModuleList(\n",
    "      (0-25): 26 x Gemma2DecoderLayer(\n",
    "        (self_attn): GemmaAttention(\n",
    "          (qkv_proj): Linear()\n",
    "          (o_proj): Linear()\n",
    "        )\n",
    "        (mlp): GemmaMLP(\n",
    "          (gate_proj): Linear()\n",
    "          (up_proj): Linear()\n",
    "          (down_proj): Linear()\n",
    "        )\n",
    "        (input_layernorm): RMSNorm()\n",
    "        (post_attention_layernorm): RMSNorm()\n",
    "        (pre_feedforward_layernorm): RMSNorm()\n",
    "        (post_feedforward_layernorm): RMSNorm()\n",
    "      )\n",
    "    )\n",
    "    (norm): RMSNorm()\n",
    "  )\n",
    "  (sampler): Sampler()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256000, ['D', 'ene', 'me', '‚ñÅ', '1', '2', '3'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hf_tokenizer.vocab), hf_tokenizer.tokenize(\"Deneme 123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b7bc5455a2445aae02f3571f0150b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sampler import Sampler\n",
    "from gemma_functions import precompute_freqs_cis, apply_rotary_emb\n",
    "from linear import Linear\n",
    "from embedding import Embedding\n",
    "from rms_norm import RMSNorm\n",
    "from gemma_mlp import GemmaMLP\n",
    "from gemma_attention import GemmaAttention\n",
    "from gemma_config import get_config_for_2b_v2\n",
    "from gemma2_decoder_layer import Gemma2DecoderLayer\n",
    "from gemma_model import GemmaModel\n",
    "from gemma_for_causal_lm import GemmaForCausalLM\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, Gemma2ForCausalLM\n",
    "\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", use_fast=True)\n",
    "hf_model = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "\n",
    "\n",
    "# Load the configuration\n",
    "config = get_config_for_2b_v2()\n",
    "config.vocab_size = len(hf_tokenizer.vocab)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "embedding = Embedding(config.vocab_size, config.hidden_size)\n",
    "# embedding.load_from_path(\"../model_weights/embedder.pth\")\n",
    "embedding.weight = hf_model.lm_head.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alibayram/Desktop/gemma_pytorch/gemma2/linear.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weight_tensor = torch.load(path)\n",
      "/Users/alibayram/Desktop/gemma_pytorch/gemma2/rms_norm.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weight_tensor = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "model_weights_path = \"../model_weights/model/\"\n",
    "\n",
    "layers = []\n",
    "for i in range(25):\n",
    "    layer_path = model_weights_path + f\"layers/layer_{i}/\"\n",
    "    attn_type = (\n",
    "        config.attn_types[i]\n",
    "        if config.attn_types is not None\n",
    "        else config.AttentionType.GLOBAL\n",
    "    )\n",
    "    num_heads = config.num_attention_heads\n",
    "    num_kv_heads = config.num_key_value_heads\n",
    "    \n",
    "    qkv_proj = Linear(\n",
    "        config.hidden_size,\n",
    "        (num_heads + 2 * num_kv_heads) * config.head_dim\n",
    "    )\n",
    "    qkv_proj.load_from_path(layer_path + \"qkv_proj.pth\")\n",
    "\n",
    "    o_proj = Linear(\n",
    "        num_heads * config.head_dim,\n",
    "        config.hidden_size\n",
    "    )\n",
    "    o_proj.load_from_path(layer_path + \"o_proj.pth\")\n",
    "\n",
    "    attention = GemmaAttention(\n",
    "        hidden_size=config.hidden_size,\n",
    "        num_heads=num_heads,\n",
    "        num_kv_heads=num_kv_heads,\n",
    "        attn_logit_softcapping=config.attn_logit_softcapping,\n",
    "        query_pre_attn_scalar=config.query_pre_attn_scalar,\n",
    "        head_dim=config.head_dim,\n",
    "        attn_type=attn_type,\n",
    "        qkv_proj=qkv_proj,\n",
    "        o_proj=o_proj,\n",
    "        sliding_window_size=config.sliding_window_size,\n",
    "    )\n",
    "    gate_proj = Linear(config.hidden_size, config.intermediate_size)\n",
    "    gate_proj.load_from_path(layer_path + \"gate_proj.pth\")\n",
    "\n",
    "    up_proj = Linear(config.hidden_size, config.intermediate_size)\n",
    "    up_proj.load_from_path(layer_path + \"up_proj.pth\")\n",
    "\n",
    "    down_proj = Linear(config.intermediate_size, config.hidden_size)\n",
    "    down_proj.load_from_path(layer_path + \"down_proj.pth\")\n",
    "\n",
    "    # Initialize the GemmaMLP\n",
    "    mlp = GemmaMLP(gate_proj=gate_proj, up_proj=up_proj, down_proj=down_proj)\n",
    "    \n",
    "    input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "    input_layernorm.load_from_path(layer_path + \"input_layernorm.pth\")\n",
    "\n",
    "    post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "    post_attention_layernorm.load_from_path(layer_path + \"post_attention_layernorm.pth\")\n",
    "\n",
    "    pre_feedforward_layernorm = (\n",
    "        RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        if config.use_pre_ffw_norm\n",
    "        else None\n",
    "    )\n",
    "    if pre_feedforward_layernorm is not None:\n",
    "        pre_feedforward_layernorm.load_from_path(layer_path + \"pre_feedforward_layernorm.pth\")\n",
    "\n",
    "    post_feedforward_layernorm = (\n",
    "        RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        if config.use_post_ffw_norm\n",
    "        else None\n",
    "    )\n",
    "    if post_feedforward_layernorm is not None:\n",
    "        post_feedforward_layernorm.load_from_path(layer_path + \"post_feedforward_layernorm.pth\")\n",
    "\n",
    "    decoder_layer = Gemma2DecoderLayer(\n",
    "        self_attn=attention,\n",
    "        mlp=mlp,\n",
    "        input_layernorm=input_layernorm,\n",
    "        post_attention_layernorm=post_attention_layernorm,\n",
    "        pre_feedforward_layernorm=pre_feedforward_layernorm,\n",
    "        post_feedforward_layernorm=post_feedforward_layernorm,\n",
    "    )\n",
    "    layers.append(decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (embedder): Embedding()\n",
       "  (model): GemmaModel(\n",
       "    (layers): ModuleList(\n",
       "      (0-24): 25 x Gemma2DecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (qkv_proj): Linear()\n",
       "          (o_proj): Linear()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear()\n",
       "          (up_proj): Linear()\n",
       "          (down_proj): Linear()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (pre_feedforward_layernorm): RMSNorm()\n",
       "        (post_feedforward_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (sampler): Sampler()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GemmaModel(config, layers)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "sampler = Sampler(config.vocab_size)\n",
    "\n",
    "gemma_model = GemmaForCausalLM(config, tokenizer=hf_tokenizer, embedding=embedding, model=model, sampler=sampler)\n",
    "gemma_model.to(\"cpu\")\n",
    "gemma_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_tokens: [[2, 235300, 25725, 1560, 4119, 42233, 2586, 473, 235336]]\n",
      "kv_caches len: 26 prompt_tokens len: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.generate(\"Nasƒ±lsƒ±n iyi misin?\", device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer_b = AutoTokenizer.from_pretrained(\"alibayram/tr_tokenizer\", is_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2304, out_features=256000, bias=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alibayram/Desktop/gemma_pytorch/gemma2/embedding.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weight_tensor = torch.load(path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = Embedding(config.vocab_size, config.hidden_size)\n",
    "embedding.load_from_path(\"../model_weights/embedder.pth\")\n",
    "hf_model.lm_head.weight = embedding.weight\n",
    "hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12312,  6055,    17,  2018,  6402,  1209,    36]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = hf_tokenizer_b.encode(\"Nasƒ±lsƒ±n, iyi misin?\", return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alibayram/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nasƒ±l sƒ±n , iyi mis in ? <eos>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = hf_model.generate(input_ids)\n",
    "hf_tokenizer_b.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xq.shape, xk.shape, xv.shape: torch.Size([2, 8, 8, 256]) torch.Size([2, 8, 4, 256]) torch.Size([2, 8, 4, 256])\n",
      "Output shape: torch.Size([2, 8, 2304])\n"
     ]
    }
   ],
   "source": [
    "# Define dimensions\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "hidden_size = config.hidden_size\n",
    "intermediate_size = config.intermediate_size\n",
    "num_heads = config.num_attention_heads\n",
    "num_kv_heads = config.num_key_value_heads\n",
    "head_dim = config.head_dim\n",
    "\n",
    "input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "pre_feedforward_layernorm = (\n",
    "    RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "    if config.use_pre_ffw_norm\n",
    "    else None\n",
    ")\n",
    "post_feedforward_layernorm = (\n",
    "    RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "    if config.use_post_ffw_norm\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Initialize decoder layer\n",
    "decoder_layer = Gemma2DecoderLayer(\n",
    "    self_attn=attention,\n",
    "    mlp=mlp,\n",
    "    input_layernorm=input_layernorm,\n",
    "    post_attention_layernorm=post_attention_layernorm,\n",
    "    pre_feedforward_layernorm=pre_feedforward_layernorm,\n",
    "    post_feedforward_layernorm=post_feedforward_layernorm,\n",
    ")\n",
    "\n",
    "# Create input tensors\n",
    "hidden_states = torch.randn(batch_size, seq_len, hidden_size)\n",
    "freqs_cis = torch.randn(seq_len, head_dim // 2, dtype=torch.complex64)\n",
    "kv_write_indices = torch.arange(seq_len)\n",
    "kv_cache = (torch.zeros(batch_size, seq_len, num_kv_heads, head_dim),\n",
    "            torch.zeros(batch_size, seq_len, num_kv_heads, head_dim))\n",
    "mask = torch.zeros(batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "# Forward pass\n",
    "output = decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    freqs_cis=freqs_cis,\n",
    "    kv_write_indices=kv_write_indices,\n",
    "    kv_cache=kv_cache,\n",
    "    mask=mask,\n",
    ")\n",
    "\n",
    "# Print output shape\n",
    "print(\"Output shape:\", output.shape)  # Expected: (batch_size, seq_len, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xq.shape, xk.shape, xv.shape: torch.Size([2, 8, 8, 256]) torch.Size([2, 8, 4, 256]) torch.Size([2, 8, 4, 256])\n",
      "Output shape: torch.Size([2, 8, 2304])\n"
     ]
    }
   ],
   "source": [
    "# Define dimensions\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "hidden_size = config.hidden_size\n",
    "intermediate_size = config.intermediate_size\n",
    "num_heads = config.num_attention_heads\n",
    "num_kv_heads = config.num_key_value_heads\n",
    "head_dim = config.head_dim\n",
    "attn_logit_softcapping = config.attn_logit_softcapping\n",
    "sliding_window_size = config.sliding_window_size\n",
    "attn_type = config.attn_types[0]  # Use the first attention type in the sequence\n",
    "\n",
    "# Create input tensors\n",
    "hidden_states = torch.randn(batch_size, seq_len, hidden_size, dtype=torch.float32)\n",
    "freqs_cis = torch.randn(seq_len, head_dim // 2, dtype=torch.complex64)  # Ensure seq_len matches\n",
    "kv_write_indices = torch.arange(seq_len)  # Example indices for key-value caching\n",
    "k_cache = torch.zeros(batch_size, seq_len, num_kv_heads, head_dim, dtype=torch.float32)\n",
    "v_cache = torch.zeros(batch_size, seq_len, num_kv_heads, head_dim, dtype=torch.float32)\n",
    "kv_cache = (k_cache, v_cache)  # Cache for keys and values\n",
    "mask = torch.zeros(batch_size, num_heads, seq_len, seq_len, dtype=torch.float32)  # Example mask\n",
    "\n",
    "# Define linear projections for QKV and output\n",
    "qkv_proj = Linear(\n",
    "    hidden_size,\n",
    "    (num_heads + 2 * num_kv_heads) * head_dim\n",
    ")\n",
    "o_proj = Linear(\n",
    "    num_heads * head_dim,\n",
    "    hidden_size\n",
    ")\n",
    "\n",
    "# Initialize the GemmaAttention module\n",
    "attention = GemmaAttention(\n",
    "    hidden_size=hidden_size,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_heads=num_kv_heads,\n",
    "    attn_logit_softcapping=attn_logit_softcapping,\n",
    "    query_pre_attn_scalar=config.query_pre_attn_scalar,\n",
    "    head_dim=head_dim,\n",
    "    attn_type=attn_type,\n",
    "    qkv_proj=qkv_proj,\n",
    "    o_proj=o_proj,\n",
    "    sliding_window_size=sliding_window_size,\n",
    ")\n",
    "\n",
    "# Forward pass through the attention module\n",
    "output = attention(\n",
    "    hidden_states=hidden_states,\n",
    "    freqs_cis=freqs_cis,\n",
    "    kv_write_indices=kv_write_indices,\n",
    "    kv_cache=kv_cache,\n",
    "    mask=mask,\n",
    ")\n",
    "\n",
    "# Print the output shape for validation\n",
    "print(\"Output shape:\", output.shape)  # Expected: (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 8, 2304])\n",
      "Output shape: torch.Size([2, 8, 2304])\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "import torch\n",
    "from linear import Linear  # Assumes `Linear` is defined in `linear.py`\n",
    "\n",
    "# Define dimensions\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "hidden_size = config.hidden_size\n",
    "intermediate_size = config.intermediate_size\n",
    "num_heads = config.num_attention_heads\n",
    "num_kv_heads = config.num_key_value_heads\n",
    "head_dim = config.head_dim\n",
    "\n",
    "# Create a random input tensor\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "gate_proj = Linear(hidden_size, intermediate_size)\n",
    "up_proj = Linear(hidden_size, intermediate_size)\n",
    "down_proj = Linear(intermediate_size, hidden_size)\n",
    "# Initialize the GemmaMLP\n",
    "mlp = GemmaMLP(gate_proj=gate_proj, up_proj=up_proj, down_proj=down_proj)\n",
    "\n",
    "# Apply the MLP to the input tensor\n",
    "output = mlp(x)\n",
    "\n",
    "# Print the shapes for verification\n",
    "print(\"Input shape:\", x.shape)  # Expected: (2, 4, 8)\n",
    "print(\"Output shape:\", output.shape)  # Expected: (2, 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 8])\n",
      "Output shape: torch.Size([2, 4, 8])\n",
      "Mean square of normalized output: tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Define dimensions\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "dim = 8  # Embedding dimension\n",
    "\n",
    "# Create a random input tensor\n",
    "x = torch.randn(batch_size, seq_len, dim)\n",
    "\n",
    "# Initialize RMSNorm\n",
    "rms_norm = RMSNorm(dim=dim, eps=1e-6, add_unit_offset=True)\n",
    "\n",
    "# Apply RMSNorm to the input tensor\n",
    "normalized_x = rms_norm(x)\n",
    "\n",
    "# Print the shapes for verification\n",
    "print(\"Input shape:\", x.shape)  # Expected: (2, 4, 8)\n",
    "print(\"Output shape:\", normalized_x.shape)  # Expected: (2, 4, 8)\n",
    "\n",
    "# Verify that the mean square of the normalized output is close to 1\n",
    "print(\"Mean square of normalized output:\", normalized_x.pow(2).mean(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(), Linear(), Sampler())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = Embedding(10, 10)\n",
    "linear = Linear(10, 10)\n",
    "sampler = Sampler(10)\n",
    "\n",
    "embedding, linear, sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 30.,  70.],\n",
      "        [ 70., 174.]])\n"
     ]
    }
   ],
   "source": [
    "in_features = 4  \n",
    "out_features = 2  \n",
    "linear = Linear(in_features, out_features) \n",
    "linear.weight.data = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=torch.float32)\n",
    "\n",
    "example_x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=torch.float32)\n",
    "example_y = linear(example_x)\n",
    "print(example_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basit √ñrnek:\n",
      "Giri≈ü ≈üekli: torch.Size([3, 4])\n",
      "√áƒ±kƒ±≈ü ≈üekli: torch.Size([3, 2])\n",
      "Aƒüƒ±rlƒ±k ≈üekli: torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# 1. Basit Kullanƒ±m √ñrneƒüi  \n",
    "def simple_example():  \n",
    "    # Model olu≈üturma  \n",
    "    in_features = 4  \n",
    "    out_features = 2  \n",
    "    linear = Linear(in_features, out_features)  \n",
    "  \n",
    "    \n",
    "    # Forward pass  \n",
    "    input_tensor = torch.randn(3, in_features)  # 3 √∂rnek, her biri 4 √∂zellikli  \n",
    "    output = linear(input_tensor)  \n",
    "    \n",
    "    print(\"Giri≈ü ≈üekli:\", input_tensor.shape)  \n",
    "    print(\"√áƒ±kƒ±≈ü ≈üekli:\", output.shape)  \n",
    "    print(\"Aƒüƒ±rlƒ±k ≈üekli:\", linear.weight.shape)  \n",
    "    \n",
    "    return linear, input_tensor, output \n",
    "\n",
    "# √ñrnekleri √ßalƒ±≈ütƒ±rma  \n",
    "print(\"Basit √ñrnek:\")  \n",
    "linear, inputs, outputs = simple_example()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frekans ≈üekli: torch.Size([16, 4])\n",
      "Query ≈üekli: torch.Size([2, 4, 16, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query, query_rotary, freqs_cis  \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# √ñrneƒüi √ßalƒ±≈ütƒ±rma  \u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m query, query_rotary, freqs \u001b[38;5;241m=\u001b[39m \u001b[43mexample_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Sonu√ßlarƒ± kontrol etme  \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m√ñrnek deƒüerler:\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mexample_usage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery ≈üekli:\u001b[39m\u001b[38;5;124m\"\u001b[39m, query\u001b[38;5;241m.\u001b[39mshape)  \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 3. Rotary embedding uygulama  \u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m query_rotary \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRotary sonrasƒ± query ≈üekli:\u001b[39m\u001b[38;5;124m\"\u001b[39m, query_rotary\u001b[38;5;241m.\u001b[39mshape)  \n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query, query_rotary, freqs_cis\n",
      "File \u001b[0;32m~/Desktop/gemma_pytorch/gemma2/gemma_functions.py:19\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(x, freqs_cis)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the rotary embedding to the query and key tensors.\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m x_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_complex(\n\u001b[1;32m     17\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack(torch\u001b[38;5;241m.\u001b[39mchunk(x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     18\u001b[0m                 dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m x_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(\u001b[43mx_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m)\u001b[38;5;241m.\u001b[39mtype_as(x)\n\u001b[1;32m     20\u001b[0m x_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(torch\u001b[38;5;241m.\u001b[39mchunk(x_out, \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m x_out \u001b[38;5;241m=\u001b[39m x_out\u001b[38;5;241m.\u001b[39mreshape(x_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     22\u001b[0m                       \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def example_usage():  \n",
    "    # Parametreleri belirleme  \n",
    "    batch_size = 2  \n",
    "    seq_length = 16  \n",
    "    num_heads = 4  \n",
    "    head_dim = 8  # Her head i√ßin boyut  \n",
    "    \n",
    "    # 1. Frekanslarƒ± hesaplama  \n",
    "    # Not: head_dim'in yarƒ±sƒ±nƒ± kullanƒ±yoruz √ß√ºnk√º kompleks sayƒ±larla √ßalƒ±≈üƒ±yoruz  \n",
    "    freqs_cis = precompute_freqs_cis(dim=head_dim, end=seq_length)  \n",
    "    print(\"Frekans ≈üekli:\", freqs_cis.shape)  \n",
    "    \n",
    "    # 2. √ñrnek query tens√∂r√º olu≈üturma  \n",
    "    query = torch.randn(batch_size, num_heads, seq_length, head_dim)  \n",
    "    print(\"Query ≈üekli:\", query.shape)  \n",
    "    \n",
    "    # 3. Rotary embedding uygulama  \n",
    "    query_rotary = apply_rotary_emb(query, freqs_cis)  \n",
    "    print(\"Rotary sonrasƒ± query ≈üekli:\", query_rotary.shape)  \n",
    "    \n",
    "    return query, query_rotary, freqs_cis  \n",
    "\n",
    "# √ñrneƒüi √ßalƒ±≈ütƒ±rma  \n",
    "query, query_rotary, freqs = example_usage()  \n",
    "\n",
    "# Sonu√ßlarƒ± kontrol etme  \n",
    "print(\"\\n√ñrnek deƒüerler:\")  \n",
    "print(\"ƒ∞lk query deƒüerleri:\", query[0, 0, 0, :5])  \n",
    "print(\"Rotary sonrasƒ± ilk deƒüerler:\", query_rotary[0, 0, 0, :5])  \n",
    "print(\"ƒ∞lk frekans deƒüerleri:\", freqs[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, seq_len, dim)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Apply rotary embeddings\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m x_with_rotary \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Print shapes for validation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected: (2, 4, 8)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/gemma_pytorch/gemma2/gemma_functions.py:19\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(x, freqs_cis)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the rotary embedding to the query and key tensors.\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m x_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_complex(\n\u001b[1;32m     17\u001b[0m     torch\u001b[38;5;241m.\u001b[39mstack(torch\u001b[38;5;241m.\u001b[39mchunk(x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     18\u001b[0m                 dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m x_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(\u001b[43mx_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m)\u001b[38;5;241m.\u001b[39mtype_as(x)\n\u001b[1;32m     20\u001b[0m x_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(torch\u001b[38;5;241m.\u001b[39mchunk(x_out, \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m x_out \u001b[38;5;241m=\u001b[39m x_out\u001b[38;5;241m.\u001b[39mreshape(x_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     22\u001b[0m                       \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define dimensions\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "dim = 8  # Must be even for rotary embeddings\n",
    "\n",
    "# Precompute rotary embeddings\n",
    "freqs_cis = precompute_freqs_cis(dim=dim, end=seq_len)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "x = torch.randn(batch_size, seq_len, dim)\n",
    "\n",
    "# Apply rotary embeddings\n",
    "x_with_rotary = apply_rotary_emb(x, freqs_cis)\n",
    "\n",
    "# Print shapes for validation\n",
    "print(\"Input shape:\", x.shape)  # Expected: (2, 4, 8)\n",
    "print(\"Output shape:\", x_with_rotary.shape)  # Expected: (2, 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next tokens: ['hello', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example vocabulary\n",
    "vocab = {0: \"<pad>\", 1: \"hello\", 2: \"world\", 3: \"I\", 4: \"am\", 5: \"a\", 6: \"token\", 7: \"example\", 8: \"for\", 9: \"you\"}\n",
    "\n",
    "# Define input parameters\n",
    "vocab_size = len(vocab)\n",
    "batch_size = 2\n",
    "seq_len = 3  # Sequence length for hidden_states\n",
    "hidden_size = 8\n",
    "temperature_value = 0.8\n",
    "top_p_value = 0.9\n",
    "top_k_value = 3\n",
    "\n",
    "# Create inputs\n",
    "embedding = torch.randn(vocab_size, hidden_size)  # (vocab_size, hidden_size)\n",
    "hidden_states = torch.randn(batch_size, seq_len, hidden_size)  # (batch_size, seq_len, hidden_size)\n",
    "output_positions = torch.tensor([i % seq_len for i in range(batch_size)])  # (batch_size,)\n",
    "temperatures = torch.tensor([temperature_value] * batch_size)  # (batch_size,)\n",
    "top_ps = torch.tensor([top_p_value] * batch_size)  # (batch_size,)\n",
    "top_ks = torch.tensor([top_k_value] * batch_size)  # (batch_size,)\n",
    "embedding_bias = torch.zeros(vocab_size)  # (vocab_size,)\n",
    "\n",
    "# Initialize the Sampler and call forward\n",
    "example = Sampler(vocab_size)\n",
    "next_token_ids, logits = example.forward(\n",
    "    embedding=embedding,\n",
    "    hidden_states=hidden_states,\n",
    "    output_positions=output_positions,\n",
    "    temperatures=temperatures,\n",
    "    top_ps=top_ps,\n",
    "    top_ks=top_ks,\n",
    "    embedding_bias=embedding_bias\n",
    ")\n",
    "\n",
    "# Print next tokens\n",
    "next_tokens = [vocab[token_id.item()] for token_id in next_token_ids]\n",
    "print(\"Next tokens:\", next_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
