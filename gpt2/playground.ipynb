{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 14, 11, 29, 0, 23, 0, 29, 1, 0, 13, 30, 29, 0, 14, 11, 29, 0, 23, 0, 29, 1, 0, 13, 30]\n",
      "ali ata bak. ali ata bak.\n"
     ]
    }
   ],
   "source": [
    "from letter_tokenizer import tokenize, detokenize\n",
    "\n",
    "text_example = \"ali ata bak. ali ata bak.\"\n",
    "tokens = tokenize(text_example)\n",
    "print(tokens)\n",
    "\n",
    "text = detokenize(tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "from gpt_config import GPTConfig\n",
    "\n",
    "test_config = GPTConfig(\n",
    "    vocab_size=32,\n",
    "    n_layer=1,  \n",
    "    n_head=1,\n",
    "    n_embd=3,\n",
    "    seq_len=12,\n",
    ")\n",
    "\n",
    "print(test_config.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_embedding): Embedding(32, 3)\n",
       "  (blocks): Sequential(\n",
       "    (0): GPTBlock(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (attn_heads): ModuleList(\n",
       "          (0): CausalSelfAttention()\n",
       "        )\n",
       "        (projection): Linear(in_features=3, out_features=3, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=12, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=12, out_features=3, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=3, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_model import GPTModel\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = GPTModel(test_config, device)\n",
    "\n",
    "parameters_count = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "    parameters_count += p.numel()\n",
    "\n",
    "print(parameters_count)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  ali ata bak.\n",
      "Predicted: ajjjjfffjjjff\n"
     ]
    }
   ],
   "source": [
    "def inference(prompt, max_new_tokens):\n",
    "    tokens = tokenize(prompt)\n",
    "    for _ in range(max_new_tokens):\n",
    "        num_tokens = len(tokens)\n",
    "        tokens_padded = tokens + [0] * (test_config.seq_len - num_tokens)\n",
    "        tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device)\n",
    "        logits = model(tokens_padded)\n",
    "        predicted_token = torch.argmax(logits[0, num_tokens-1, :]).item()\n",
    "        tokens.append(predicted_token)\n",
    "    return detokenize(tokens)\n",
    "\n",
    "print(\"Original: \", text_example[:test_config.seq_len])\n",
    "row_model_prediction = inference(text_example[0], max_new_tokens=test_config.seq_len)\n",
    "print(\"Predicted:\", row_model_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([1, 12])\n",
      "Output Shape torch.Size([1, 12])\n",
      "Input Example:\n",
      "tensor([[ 0, 14, 11, 29,  0, 23,  0, 29,  1,  0, 13, 30]], device='mps:0')\n",
      "Output Example:\n",
      "tensor([[14, 11, 29,  0, 23,  0, 29,  1,  0, 13, 30, 29]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "with open(\"tr_texts_400.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    tr_texts = file.read()\n",
    "\n",
    "# text_example = tr_texts\n",
    "\n",
    "tokenized_text = tokenize(text_example)\n",
    "\n",
    "def get_dataset(num_examples, context_window_length, test_split=0.1):\n",
    "    input_blocks = [] # List to store input sequences\n",
    "    target_blocks = [] # List to store target sequences\n",
    "\n",
    "    # Use a sliding window to create input/target sequences\n",
    "    for i in range(0, len(tokenized_text), context_window_length + 1):\n",
    "        block = tokenized_text[i:i+context_window_length+ 1]\n",
    "        \n",
    "        # Skip blocks that are too short\n",
    "        if len(block) < context_window_length + 1:\n",
    "            continue\n",
    "\n",
    "        input_seq = block[:-1]  \n",
    "        target_seq = block[1:]  \n",
    "\n",
    "        input_blocks.append(input_seq)\n",
    "        target_blocks.append(target_seq)\n",
    "        \n",
    "        # Stop if we have enough examples\n",
    "        if len(input_blocks) >= num_examples:\n",
    "            break\n",
    "\n",
    "    # Convert to tensors for pytorch and move to gpu\n",
    "    inputs = torch.tensor(input_blocks, dtype=torch.long).to(device)\n",
    "    targets = torch.tensor(target_blocks, dtype=torch.long).to(device)\n",
    "\n",
    "    # Calculate train/test split point\n",
    "    split_idx = int(num_examples * (1 - test_split))\n",
    "\n",
    "    # Split into train/test\n",
    "    train_inputs = inputs[:split_idx]\n",
    "    train_targets = targets[:split_idx]\n",
    "    test_inputs = inputs[split_idx:]\n",
    "    test_targets = targets[split_idx:]\n",
    "    return train_inputs, train_targets, test_inputs, test_targets\n",
    "\n",
    "# Get a small dataset\n",
    "i, o, _, _ = get_dataset(4, test_config.seq_len, 0)\n",
    "print(\"Input Shape\", i.shape)\n",
    "print(\"Output Shape\", o.shape)\n",
    "print(\"Input Example:\")\n",
    "print(i)\n",
    "print(\"Output Example:\")\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2/2000\t\tLoss: 3.790171\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: ajjjjfffjjjff\tRow: ajjjjfffjjjff\n",
      "Step 152/2000\t\tLoss: 3.199797\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: affjuffffffff\tRow: ajjjjfffjjjff\n",
      "Step 302/2000\t\tLoss: 2.604598\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: alfkultllk.tl\tRow: ajjjjfffjjjff\n",
      "Step 452/2000\t\tLoss: 2.063809\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: a  khit    it\tRow: ajjjjfffjjjff\n",
      "Step 602/2000\t\tLoss: 1.697719\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: a i      i i \tRow: ajjjjfffjjjff\n",
      "Step 752/2000\t\tLoss: 1.425092\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: a i     tk.  \tRow: ajjjjfffjjjff\n",
      "Step 902/2000\t\tLoss: 1.281379\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: aai  aa tk. a\tRow: ajjjjfffjjjff\n",
      "Step 1052/2000\t\tLoss: 1.130798\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: aaiaka  tk. a\tRow: ajjjjfffjjjff\n",
      "Step 1202/2000\t\tLoss: 0.994100\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: ali ata tk. a\tRow: ajjjjfffjjjff\n",
      "Step 1352/2000\t\tLoss: 0.875356\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: ali ata tk.ta\tRow: ajjjjfffjjjff\n",
      "Step 1502/2000\t\tLoss: 0.811795\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: ali ata tk.ta\tRow: ajjjjfffjjjff\n",
      "Step 1652/2000\t\tLoss: 0.767525\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: ali ata tk.ta\tRow: ajjjjfffjjjff\n",
      "Step 1802/2000\t\tLoss: 0.721862\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: ali ata tk.ta\tRow: ajjjjfffjjjff\n",
      "Step 1952/2000\t\tLoss: 0.648250\t\tLR: 0.0005\n",
      "Original: ali ata bak.\tPredicted: ali ata bak. \tRow: ajjjjfffjjjff\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 1\n",
    "num_steps = 2000\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Define Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.2, patience=20, min_lr=5e-6, threshold=1e-4)\n",
    "\n",
    "# Training loop\n",
    "i = 1\n",
    "losses = []\n",
    "\n",
    "train_inputs, train_targets, _, _ = get_dataset(100, test_config.seq_len, 0)\n",
    "\n",
    "while i < num_steps:\n",
    "    for j in range(0, len(train_inputs), batch_size):\n",
    "        x = train_inputs[j:j+batch_size]\n",
    "        y = train_targets[j:j+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "\n",
    "        loss = loss.item()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "   \n",
    "        # Print the average loss for the epoch\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        if i % 150 == 1:\n",
    "            print(f\"Step {i+1}/{num_steps}\\t\\tLoss: {loss:.6f}\\t\\tLR: {lr}\")\n",
    "            print(f\"Original: {text_example[:test_config.seq_len]}\\tPredicted: {inference(text_example[0], max_new_tokens=test_config.seq_len)}\\tRow: {row_model_prediction}\")\n",
    "\n",
    "        i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
